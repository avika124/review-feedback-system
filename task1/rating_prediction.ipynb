{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Star Rating Prediction via LLM Prompting\n",
    "\n",
    "This notebook implements three different prompting approaches to classify Yelp reviews into 1-5 star ratings using Large Language Models (LLMs).\n",
    "\n",
    "## Objectives\n",
    "1. Compare Zero-Shot, Few-Shot, and Chain-of-Thought prompting approaches\n",
    "2. Evaluate accuracy, JSON validity, and latency metrics\n",
    "3. Identify the best performing approach for rating prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../task2/backend/.env') \n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load API key from environment\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set GEMINI_API_KEY environment variable\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "print(\"✓ Setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading & Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Dataset file not found. Creating sample dataset for demonstration...\n",
      "✓ Sample dataset created: 200 rows\n",
      "\n",
      "Sample reviews:\n",
      "                                                text  stars\n",
      "0  Amazing food and great service! Will definitel...      5\n",
      "1  Terrible experience. Food was cold and service...      1\n",
      "2          Pretty good overall, but could be better.      3\n",
      "3   Best restaurant in town! Everything was perfect.      5\n",
      "4            Not worth the money. Very disappointed.      2\n"
     ]
    }
   ],
   "source": [
    "# Load the Yelp Reviews dataset\n",
    "# Note: Download from https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\n",
    "# Place the CSV file in the task1/ directory\n",
    "\n",
    "dataset_path = 'yelp_reviews.csv'  # Update with actual filename\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"✓ Dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Dataset file not found. Creating sample dataset for demonstration...\")\n",
    "    # Create a sample dataset structure\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            \"Amazing food and great service! Will definitely come back.\",\n",
    "            \"Terrible experience. Food was cold and service was slow.\",\n",
    "            \"Pretty good overall, but could be better.\",\n",
    "            \"Best restaurant in town! Everything was perfect.\",\n",
    "            \"Not worth the money. Very disappointed.\"\n",
    "        ] * 40,  # 200 samples\n",
    "        'stars': [5, 1, 3, 5, 2] * 40\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"✓ Sample dataset created: {len(df)} rows\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample reviews:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test set prepared: 200 reviews\n",
      "\n",
      "Rating distribution:\n",
      "stars\n",
      "1    40\n",
      "2    40\n",
      "3    40\n",
      "5    80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample ~200 rows and split into test set\n",
    "np.random.seed(42)\n",
    "sample_size = min(200, len(df))\n",
    "\n",
    "if len(df) > sample_size:\n",
    "    df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "\n",
    "# Use all sampled data as test set for consistent evaluation\n",
    "test_set = df_sample.copy()\n",
    "\n",
    "print(f\"✓ Test set prepared: {len(test_set)} reviews\")\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(test_set['stars'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def call_llm(prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Call Gemini API with retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "\n",
    "def parse_json_response(response: str) -> Dict:\n",
    "    \"\"\"Parse JSON from LLM response, handling markdown code blocks\"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    # Remove markdown code blocks if present\n",
    "    response = response.strip()\n",
    "    if response.startswith('```'):\n",
    "        # Extract JSON from code block\n",
    "        lines = response.split('\\n')\n",
    "        json_lines = []\n",
    "        in_json = False\n",
    "        for line in lines:\n",
    "            if line.strip().startswith('```'):\n",
    "                in_json = not in_json\n",
    "                continue\n",
    "            if in_json or (not response.startswith('```json') and not response.startswith('```')):\n",
    "                json_lines.append(line)\n",
    "        response = '\\n'.join(json_lines)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract JSON object from text\n",
    "        import re\n",
    "        json_match = re.search(r'\\{[^{}]*\"predicted_stars\"[^{}]*\\}', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                return json.loads(json_match.group())\n",
    "            except:\n",
    "                pass\n",
    "        return None\n",
    "\n",
    "def evaluate_predictions(actual: List[int], predicted: List[int], latencies: List[float]) -> Dict:\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    # Exact match accuracy\n",
    "    exact_matches = sum(1 for a, p in zip(actual, predicted) if a == p)\n",
    "    exact_accuracy = exact_matches / len(actual) if actual else 0\n",
    "    \n",
    "    # Within ±1 accuracy\n",
    "    within_one = sum(1 for a, p in zip(actual, predicted) if abs(a - p) <= 1)\n",
    "    within_one_accuracy = within_one / len(actual) if actual else 0\n",
    "    \n",
    "    # JSON validity rate\n",
    "    valid_responses = sum(1 for p in predicted if p is not None)\n",
    "    validity_rate = valid_responses / len(predicted) if predicted else 0\n",
    "    \n",
    "    # Latency metrics\n",
    "    avg_latency = np.mean(latencies) if latencies else 0\n",
    "    std_latency = np.std(latencies) if latencies else 0\n",
    "    \n",
    "    return {\n",
    "        'exact_accuracy': exact_accuracy,\n",
    "        'within_one_accuracy': within_one_accuracy,\n",
    "        'validity_rate': validity_rate,\n",
    "        'avg_latency': avg_latency,\n",
    "        'std_latency': std_latency,\n",
    "        'total_predictions': len(predicted),\n",
    "        'valid_predictions': valid_responses\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Approach 1: Zero-Shot Classification\n",
    "\n",
    "**Rationale**: Direct instruction to classify without examples. Simple and fast, tests the model's inherent understanding of rating scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot prompt template:\n",
      "Classify the following Yelp review into a star rating from 1 to 5, where:\n",
      "- 1 star = Very negative, terrible experience\n",
      "- 2 stars = Negative, poor experience\n",
      "- 3 stars = Neutral, average experience\n",
      "- 4 stars = Positive, good experience\n",
      "- 5 stars = Very positive, excellent experience\n",
      "\n",
      "Review: \"Sample review text\"\n",
      "\n",
      "Respond with a JSON object in this exact format:\n",
      "{\n",
      "  \"predicted_stars\": <number between 1 and 5>,\n",
      "  \"explanation\": \"<brief reasoning for the assigned rating>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def zero_shot_prompt(review_text: str) -> str:\n",
    "    \"\"\"Generate zero-shot classification prompt\"\"\"\n",
    "    prompt = f\"\"\"Classify the following Yelp review into a star rating from 1 to 5, where:\n",
    "- 1 star = Very negative, terrible experience\n",
    "- 2 stars = Negative, poor experience\n",
    "- 3 stars = Neutral, average experience\n",
    "- 4 stars = Positive, good experience\n",
    "- 5 stars = Very positive, excellent experience\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Respond with a JSON object in this exact format:\n",
    "{{\n",
    "  \"predicted_stars\": <number between 1 and 5>,\n",
    "  \"explanation\": \"<brief reasoning for the assigned rating>\"\n",
    "}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Zero-shot prompt template:\")\n",
    "print(zero_shot_prompt(\"Sample review text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Zero-Shot Classification...\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "  Processed 20/200 reviews...\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Error after 3 attempts: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    }
   ],
   "source": [
    "# Run zero-shot classification on test set\n",
    "print(\"Running Zero-Shot Classification...\")\n",
    "zero_shot_results = []\n",
    "zero_shot_latencies = []\n",
    "\n",
    "for idx, row in test_set.iterrows():\n",
    "    review_text = str(row['text'])\n",
    "    actual_rating = int(row['stars'])\n",
    "    \n",
    "    prompt = zero_shot_prompt(review_text)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = call_llm(prompt)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    zero_shot_latencies.append(latency)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and 'predicted_stars' in parsed:\n",
    "        predicted = int(parsed['predicted_stars'])\n",
    "        explanation = parsed.get('explanation', '')\n",
    "    else:\n",
    "        predicted = None\n",
    "        explanation = ''\n",
    "    \n",
    "    zero_shot_results.append({\n",
    "        'actual': actual_rating,\n",
    "        'predicted': predicted,\n",
    "        'explanation': explanation\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{len(test_set)} reviews...\")\n",
    "\n",
    "print(\"✓ Zero-Shot classification complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate zero-shot results\n",
    "zero_shot_predicted = [r['predicted'] if r['predicted'] is not None else 0 for r in zero_shot_results]\n",
    "zero_shot_actual = [r['actual'] for r in zero_shot_results]\n",
    "\n",
    "zero_shot_metrics = evaluate_predictions(\n",
    "    zero_shot_actual,\n",
    "    zero_shot_predicted,\n",
    "    zero_shot_latencies\n",
    ")\n",
    "\n",
    "print(\"Zero-Shot Results:\")\n",
    "print(f\"  Exact Accuracy: {zero_shot_metrics['exact_accuracy']:.2%}\")\n",
    "print(f\"  Within ±1 Accuracy: {zero_shot_metrics['within_one_accuracy']:.2%}\")\n",
    "print(f\"  JSON Validity Rate: {zero_shot_metrics['validity_rate']:.2%}\")\n",
    "print(f\"  Average Latency: {zero_shot_metrics['avg_latency']:.2f}s\")\n",
    "print(f\"  Std Dev Latency: {zero_shot_metrics['std_latency']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Approach 2: Few-Shot Classification\n",
    "\n",
    "**Rationale**: Provide examples for each rating category to guide the model's understanding. Shows the model what good classifications look like through demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt(review_text: str) -> str:\n",
    "    \"\"\"Generate few-shot classification prompt with examples\"\"\"\n",
    "    examples = \"\"\"\n",
    "Examples:\n",
    "\n",
    "Review: \"Terrible service, food was cold and tasteless. Never coming back.\"\n",
    "Rating: 1 star\n",
    "Explanation: Very negative experience with poor service and food quality.\n",
    "\n",
    "Review: \"Not great, but not terrible either. Food was okay but service was slow.\"\n",
    "Rating: 2 stars\n",
    "Explanation: Negative experience overall with below-average service.\n",
    "\n",
    "Review: \"It was fine. Nothing special, nothing terrible. Average experience.\"\n",
    "Rating: 3 stars\n",
    "Explanation: Neutral experience, neither positive nor negative.\n",
    "\n",
    "Review: \"Good food and friendly staff. Had a nice time, would recommend.\"\n",
    "Rating: 4 stars\n",
    "Explanation: Positive experience with good food and service.\n",
    "\n",
    "Review: \"Absolutely amazing! Best restaurant experience ever. Perfect food, perfect service, perfect atmosphere!\"\n",
    "Rating: 5 stars\n",
    "Explanation: Exceptional experience across all aspects.\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Classify the following Yelp review into a star rating from 1 to 5.\n",
    "{examples}\n",
    "\n",
    "Now classify this review:\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Respond with a JSON object in this exact format:\n",
    "{{\n",
    "  \"predicted_stars\": <number between 1 and 5>,\n",
    "  \"explanation\": \"<brief reasoning for the assigned rating>\"\n",
    "}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Few-shot prompt template (showing structure):\")\n",
    "print(\"Includes 5 examples (one for each rating)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run few-shot classification on test set\n",
    "print(\"Running Few-Shot Classification...\")\n",
    "few_shot_results = []\n",
    "few_shot_latencies = []\n",
    "\n",
    "for idx, row in test_set.iterrows():\n",
    "    review_text = str(row['text'])\n",
    "    actual_rating = int(row['stars'])\n",
    "    \n",
    "    prompt = few_shot_prompt(review_text)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = call_llm(prompt)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    few_shot_latencies.append(latency)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and 'predicted_stars' in parsed:\n",
    "        predicted = int(parsed['predicted_stars'])\n",
    "        explanation = parsed.get('explanation', '')\n",
    "    else:\n",
    "        predicted = None\n",
    "        explanation = ''\n",
    "    \n",
    "    few_shot_results.append({\n",
    "        'actual': actual_rating,\n",
    "        'predicted': predicted,\n",
    "        'explanation': explanation\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{len(test_set)} reviews...\")\n",
    "\n",
    "print(\"✓ Few-Shot classification complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate few-shot results\n",
    "few_shot_predicted = [r['predicted'] if r['predicted'] is not None else 0 for r in few_shot_results]\n",
    "few_shot_actual = [r['actual'] for r in few_shot_results]\n",
    "\n",
    "few_shot_metrics = evaluate_predictions(\n",
    "    few_shot_actual,\n",
    "    few_shot_predicted,\n",
    "    few_shot_latencies\n",
    ")\n",
    "\n",
    "print(\"Few-Shot Results:\")\n",
    "print(f\"  Exact Accuracy: {few_shot_metrics['exact_accuracy']:.2%}\")\n",
    "print(f\"  Within ±1 Accuracy: {few_shot_metrics['within_one_accuracy']:.2%}\")\n",
    "print(f\"  JSON Validity Rate: {few_shot_metrics['validity_rate']:.2%}\")\n",
    "print(f\"  Average Latency: {few_shot_metrics['avg_latency']:.2f}s\")\n",
    "print(f\"  Std Dev Latency: {few_shot_metrics['std_latency']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Approach 3: Chain-of-Thought (CoT) with Structured Analysis\n",
    "\n",
    "**Rationale**: Break down the analysis into explicit reasoning steps. This guides the model through a structured thought process before making the final classification, potentially improving accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_prompt(review_text: str) -> str:\n",
    "    \"\"\"Generate chain-of-thought classification prompt\"\"\"\n",
    "    prompt = f\"\"\"Analyze the following Yelp review step by step, then classify it into a star rating (1-5).\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1. **Sentiment Analysis**: Determine the overall sentiment (very negative, negative, neutral, positive, very positive)\n",
    "\n",
    "2. **Aspect Extraction**: Identify key aspects mentioned (food quality, service, ambiance, value, etc.) and note the sentiment for each\n",
    "\n",
    "3. **Overall Satisfaction Assessment**: Based on the sentiment and aspects, assess the overall customer satisfaction level\n",
    "\n",
    "4. **Rating Assignment**: Map the satisfaction level to a star rating:\n",
    "   - Very negative → 1 star\n",
    "   - Negative → 2 stars\n",
    "   - Neutral → 3 stars\n",
    "   - Positive → 4 stars\n",
    "   - Very positive → 5 stars\n",
    "\n",
    "Respond with a JSON object in this exact format:\n",
    "{{\n",
    "  \"predicted_stars\": <number between 1 and 5>,\n",
    "  \"explanation\": \"<brief reasoning that mentions your analysis steps>\"\n",
    "}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Chain-of-Thought prompt template:\")\n",
    "print(\"Includes structured reasoning steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain-of-thought classification on test set\n",
    "print(\"Running Chain-of-Thought Classification...\")\n",
    "cot_results = []\n",
    "cot_latencies = []\n",
    "\n",
    "for idx, row in test_set.iterrows():\n",
    "    review_text = str(row['text'])\n",
    "    actual_rating = int(row['stars'])\n",
    "    \n",
    "    prompt = chain_of_thought_prompt(review_text)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = call_llm(prompt)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    cot_latencies.append(latency)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and 'predicted_stars' in parsed:\n",
    "        predicted = int(parsed['predicted_stars'])\n",
    "        explanation = parsed.get('explanation', '')\n",
    "    else:\n",
    "        predicted = None\n",
    "        explanation = ''\n",
    "    \n",
    "    cot_results.append({\n",
    "        'actual': actual_rating,\n",
    "        'predicted': predicted,\n",
    "        'explanation': explanation\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{len(test_set)} reviews...\")\n",
    "\n",
    "print(\"✓ Chain-of-Thought classification complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate chain-of-thought results\n",
    "cot_predicted = [r['predicted'] if r['predicted'] is not None else 0 for r in cot_results]\n",
    "cot_actual = [r['actual'] for r in cot_results]\n",
    "\n",
    "cot_metrics = evaluate_predictions(\n",
    "    cot_actual,\n",
    "    cot_predicted,\n",
    "    cot_latencies\n",
    ")\n",
    "\n",
    "print(\"Chain-of-Thought Results:\")\n",
    "print(f\"  Exact Accuracy: {cot_metrics['exact_accuracy']:.2%}\")\n",
    "print(f\"  Within ±1 Accuracy: {cot_metrics['within_one_accuracy']:.2%}\")\n",
    "print(f\"  JSON Validity Rate: {cot_metrics['validity_rate']:.2%}\")\n",
    "print(f\"  Average Latency: {cot_metrics['avg_latency']:.2f}s\")\n",
    "print(f\"  Std Dev Latency: {cot_metrics['std_latency']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate chain-of-thought results\n",
    "cot_predicted = [r['predicted'] if r['predicted'] is not None else 0 for r in cot_results]\n",
    "cot_actual = [r['actual'] for r in cot_results]\n",
    "\n",
    "cot_metrics = evaluate_predictions(\n",
    "    cot_actual,\n",
    "    cot_predicted,\n",
    "    cot_latencies\n",
    ")\n",
    "\n",
    "print(\"Chain-of-Thought Results:\")\n",
    "print(f\"  Exact Accuracy: {cot_metrics['exact_accuracy']:.2%}\")\n",
    "print(f\"  Within ±1 Accuracy: {cot_metrics['within_one_accuracy']:.2%}\")\n",
    "print(f\"  JSON Validity Rate: {cot_metrics['validity_rate']:.2%}\")\n",
    "print(f\"  Average Latency: {cot_metrics['avg_latency']:.2f}s\")\n",
    "print(f\"  Std Dev Latency: {cot_metrics['std_latency']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison & Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Approach': ['Zero-Shot', 'Few-Shot', 'Chain-of-Thought'],\n",
    "    'Exact Accuracy': [\n",
    "        zero_shot_metrics['exact_accuracy'],\n",
    "        few_shot_metrics['exact_accuracy'],\n",
    "        cot_metrics['exact_accuracy']\n",
    "    ],\n",
    "    'Within ±1 Accuracy': [\n",
    "        zero_shot_metrics['within_one_accuracy'],\n",
    "        few_shot_metrics['within_one_accuracy'],\n",
    "        cot_metrics['within_one_accuracy']\n",
    "    ],\n",
    "    'JSON Validity Rate': [\n",
    "        zero_shot_metrics['validity_rate'],\n",
    "        few_shot_metrics['validity_rate'],\n",
    "        cot_metrics['validity_rate']\n",
    "    ],\n",
    "    'Avg Latency (s)': [\n",
    "        zero_shot_metrics['avg_latency'],\n",
    "        few_shot_metrics['avg_latency'],\n",
    "        cot_metrics['avg_latency']\n",
    "    ],\n",
    "    'Std Dev Latency (s)': [\n",
    "        zero_shot_metrics['std_latency'],\n",
    "        few_shot_metrics['std_latency'],\n",
    "        cot_metrics['std_latency']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Exact Accuracy'] = comparison_df['Exact Accuracy'].apply(lambda x: f\"{x:.2%}\")\n",
    "comparison_df['Within ±1 Accuracy'] = comparison_df['Within ±1 Accuracy'].apply(lambda x: f\"{x:.2%}\")\n",
    "comparison_df['JSON Validity Rate'] = comparison_df['JSON Validity Rate'].apply(lambda x: f\"{x:.2%}\")\n",
    "comparison_df['Avg Latency (s)'] = comparison_df['Avg Latency (s)'].apply(lambda x: f\"{x:.2f}\")\n",
    "comparison_df['Std Dev Latency (s)'] = comparison_df['Std Dev Latency (s)'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(\"Comparison Table:\")\n",
    "print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "approaches = ['Zero-Shot', 'Few-Shot', 'Chain-of-Thought']\n",
    "exact_acc = [zero_shot_metrics['exact_accuracy'], few_shot_metrics['exact_accuracy'], cot_metrics['exact_accuracy']]\n",
    "within_one_acc = [zero_shot_metrics['within_one_accuracy'], few_shot_metrics['within_one_accuracy'], cot_metrics['within_one_accuracy']]\n",
    "\n",
    "x = np.arange(len(approaches))\n",
    "width = 0.35\n",
    "axes[0, 0].bar(x - width/2, exact_acc, width, label='Exact Match', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, within_one_acc, width, label='Within ±1', alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(approaches)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "\n",
    "# 2. Latency Comparison\n",
    "avg_latencies = [zero_shot_metrics['avg_latency'], few_shot_metrics['avg_latency'], cot_metrics['avg_latency']]\n",
    "axes[0, 1].bar(approaches, avg_latencies, alpha=0.8, color='orange')\n",
    "axes[0, 1].set_ylabel('Average Latency (seconds)')\n",
    "axes[0, 1].set_title('Latency Comparison')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. JSON Validity Rate\n",
    "validity_rates = [zero_shot_metrics['validity_rate'], few_shot_metrics['validity_rate'], cot_metrics['validity_rate']]\n",
    "axes[1, 0].bar(approaches, validity_rates, alpha=0.8, color='green')\n",
    "axes[1, 0].set_ylabel('Validity Rate')\n",
    "axes[1, 0].set_title('JSON Validity Rate')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Confusion Matrix for best approach\n",
    "# Determine best approach\n",
    "best_idx = np.argmax(exact_acc)\n",
    "best_name = approaches[best_idx]\n",
    "if best_idx == 0:\n",
    "    best_pred = zero_shot_predicted\n",
    "    best_actual = zero_shot_actual\n",
    "elif best_idx == 1:\n",
    "    best_pred = few_shot_predicted\n",
    "    best_actual = few_shot_actual\n",
    "else:\n",
    "    best_pred = cot_predicted\n",
    "    best_actual = cot_actual\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(best_actual, best_pred, labels=[1, 2, 3, 4, 5])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1], \n",
    "            xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
    "axes[1, 1].set_xlabel('Predicted')\n",
    "axes[1, 1].set_ylabel('Actual')\n",
    "axes[1, 1].set_title(f'Confusion Matrix: {best_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion & Conclusions\n",
    "\n",
    "### Approach Comparison\n",
    "\n",
    "**Zero-Shot Classification:**\n",
    "- **Why chosen**: Tests the model's inherent understanding without guidance\n",
    "- **Strengths**: Fastest approach, minimal prompt overhead\n",
    "- **Weaknesses**: May lack context for edge cases\n",
    "- **Improvements made**: Clear rating scale definition in prompt\n",
    "\n",
    "**Few-Shot Classification:**\n",
    "- **Why chosen**: Provides concrete examples to guide the model\n",
    "- **Strengths**: Demonstrates expected output format and reasoning\n",
    "- **Weaknesses**: Longer prompts, slightly slower\n",
    "- **Improvements made**: Included diverse examples covering all rating levels\n",
    "\n",
    "**Chain-of-Thought (CoT) Classification:**\n",
    "- **Why chosen**: Forces structured reasoning before classification\n",
    "- **Strengths**: More thorough analysis, potentially better accuracy\n",
    "- **Weaknesses**: Longest prompts, highest latency\n",
    "- **Improvements made**: Clear step-by-step reasoning framework\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Best Performing Approach**: [Will be determined after running]\n",
    "   - Based on exact accuracy, validity rate, and latency trade-offs\n",
    "\n",
    "2. **JSON Validity**: All approaches should maintain high validity rates with proper parsing\n",
    "\n",
    "3. **Latency Considerations**: Zero-shot is fastest, but accuracy may justify additional latency in other approaches\n",
    "\n",
    "4. **Accuracy Patterns**: \n",
    "   - Exact match accuracy shows precise classification ability\n",
    "   - Within ±1 accuracy shows if model understands rating scale even if not exact\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- For production: Choose approach based on accuracy vs latency trade-off\n",
    "- Consider hybrid approach: Use faster method for simple cases, CoT for ambiguous reviews\n",
    "- Fine-tune prompts based on domain-specific review patterns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
